{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "CV_ResNet_AlexNet",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyA-EhaB/FEDIS_Tasks/blob/main/CV_ResNet_AlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- AlexNet (2012): First breakthrough CNN. Shallow (8 layers), fast, weaker accuracy by today’s standards.\n",
        "\n",
        "- ResNet152 (2015): Very deep (152 layers), uses skip connections so it can train without vanishing gradients. Much more accurate, but heavier and slower."
      ],
      "metadata": {
        "id": "66OELIEk8shu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:49:59.703401Z",
          "iopub.execute_input": "2025-08-24T01:49:59.703586Z",
          "iopub.status.idle": "2025-08-24T01:50:06.577373Z",
          "shell.execute_reply.started": "2025-08-24T01:49:59.703569Z",
          "shell.execute_reply": "2025-08-24T01:50:06.57661Z"
        },
        "id": "dJVNR9ih8shv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing (Transforms)\n",
        "### Pretrained models expect images:\n",
        "##### - size: 224x224\n",
        "##### - normalized using ImageNet's mean & std"
      ],
      "metadata": {
        "id": "_CAupyPO8shw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:06.578881Z",
          "iopub.execute_input": "2025-08-24T01:50:06.579349Z",
          "iopub.status.idle": "2025-08-24T01:50:06.583234Z",
          "shell.execute_reply.started": "2025-08-24T01:50:06.57933Z",
          "shell.execute_reply": "2025-08-24T01:50:06.582471Z"
        },
        "id": "4fGoW3rb8shx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'transforms.ToTensor()'**\n",
        "\n",
        "What does it do?\n",
        "- PyTorch expects images as tensors in a specific format:\n",
        "\n",
        "- Channels first → (C, H, W)\n",
        "\n",
        "- C = channels (3 for RGB), H = height, W = width\n",
        "\n",
        "- But most image libraries (like PIL, OpenCV, NumPy) store them as:\n",
        "\n",
        "- Channels last → (H, W, C)\n",
        "\n",
        "- 👉 transforms.ToTensor() changes the order from (H, W, C) to (C, H, W)."
      ],
      "metadata": {
        "id": "4eoZGyV-8shx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root = './data', train = True, download = True,\n",
        "    transform = transform\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:06.584146Z",
          "iopub.execute_input": "2025-08-24T01:50:06.584489Z",
          "iopub.status.idle": "2025-08-24T01:50:10.764404Z",
          "shell.execute_reply.started": "2025-08-24T01:50:06.584463Z",
          "shell.execute_reply": "2025-08-24T01:50:10.763812Z"
        },
        "id": "p6CQRSEI8shy",
        "outputId": "27b61973-9f53-41ae-f444-2e54b8eab7f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 170M/170M [00:01<00:00, 106MB/s]  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader  =torch.utils.data.DataLoader(trainset, batch_size = 32,shuffle = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:10.765074Z",
          "iopub.execute_input": "2025-08-24T01:50:10.765327Z",
          "iopub.status.idle": "2025-08-24T01:50:10.769421Z",
          "shell.execute_reply.started": "2025-08-24T01:50:10.765302Z",
          "shell.execute_reply": "2025-08-24T01:50:10.768759Z"
        },
        "id": "MJ6CCFtv8shz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True,\n",
        "    transform = transform )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:10.771537Z",
          "iopub.execute_input": "2025-08-24T01:50:10.77211Z",
          "iopub.status.idle": "2025-08-24T01:50:11.625433Z",
          "shell.execute_reply.started": "2025-08-24T01:50:10.77208Z",
          "shell.execute_reply": "2025-08-24T01:50:11.624899Z"
        },
        "id": "YaGmZYge8shz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "testloader  =torch.utils.data.DataLoader(testset, batch_size = 32,shuffle = False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:11.626141Z",
          "iopub.execute_input": "2025-08-24T01:50:11.626392Z",
          "iopub.status.idle": "2025-08-24T01:50:11.63007Z",
          "shell.execute_reply.started": "2025-08-24T01:50:11.626363Z",
          "shell.execute_reply": "2025-08-24T01:50:11.629399Z"
        },
        "id": "0fjc3jLc8sh0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "classes = trainset.classes\n",
        "print(classes)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:11.630824Z",
          "iopub.execute_input": "2025-08-24T01:50:11.63113Z",
          "iopub.status.idle": "2025-08-24T01:50:11.641494Z",
          "shell.execute_reply.started": "2025-08-24T01:50:11.631112Z",
          "shell.execute_reply": "2025-08-24T01:50:11.640868Z"
        },
        "id": "0K_7zgFQ8sh0",
        "outputId": "73a1b5b4-b40e-4d24-e39d-93f2635731c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models"
      ],
      "metadata": {
        "id": "t56ag35P8sh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "# ResNet152\n",
        "resnet = models.resnet152(pretrained = True)\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, len(classes))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:11.642174Z",
          "iopub.execute_input": "2025-08-24T01:50:11.642379Z",
          "iopub.status.idle": "2025-08-24T01:50:13.995449Z",
          "shell.execute_reply.started": "2025-08-24T01:50:11.642362Z",
          "shell.execute_reply": "2025-08-24T01:50:13.99487Z"
        },
        "id": "xlHz0wJ-8sh1",
        "outputId": "3b6c61d7-2733-4529-dc5a-91231aafb749"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n100%|██████████| 230M/230M [00:01<00:00, 210MB/s]  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "alexnet = models.alexnet(pretrained = True)\n",
        "for param in alexnet.parameters():\n",
        "    param.require_grad = False\n",
        "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, len(classes))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:13.996419Z",
          "iopub.execute_input": "2025-08-24T01:50:13.996639Z",
          "iopub.status.idle": "2025-08-24T01:50:15.814729Z",
          "shell.execute_reply.started": "2025-08-24T01:50:13.996616Z",
          "shell.execute_reply": "2025-08-24T01:50:15.813828Z"
        },
        "id": "T4m5UEa48sh1",
        "outputId": "b5f98d9e-813a-42ff-d447-064b878e3653"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 215MB/s]  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- param.requires_grad = False\n",
        "\n",
        "In PyTorch, every parameter (weights & biases) has a flag requires_grad.\n",
        "\n",
        "If True → gradients are computed during backpropagation (so it can be updated).\n",
        "\n",
        "If False → parameter is frozen (not updated during training).\n",
        "\n",
        "👉 Why do this?\n",
        "\n",
        "We are doing transfer learning.\n",
        "\n",
        "The pretrained AlexNet already knows useful features (edges, shapes, textures) from ImageNet.\n",
        "\n",
        "We don’t want to “retrain” those millions of parameters (takes too long, needs huge data).\n",
        "\n",
        "So we freeze them by setting requires_grad = False."
      ],
      "metadata": {
        "id": "vyT3rhSU8sh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 5. Training\n",
        "# ==============================\n",
        "# What happens in each epoch:\n",
        "# - Set model to training mode\n",
        "# - Loop through batches\n",
        "# - Forward pass → get predictions\n",
        "# - Compute loss (CrossEntropy)\n",
        "# - Backward pass → update weights\n",
        "# - Print average loss per epoch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:15.815713Z",
          "iopub.execute_input": "2025-08-24T01:50:15.816293Z",
          "iopub.status.idle": "2025-08-24T01:50:15.819551Z",
          "shell.execute_reply.started": "2025-08-24T01:50:15.816273Z",
          "shell.execute_reply": "2025-08-24T01:50:15.818809Z"
        },
        "id": "ApkBu3QJ8sh2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, trainlaoder, epochs = 5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images , labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss +=loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(trainloader):.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:15.820321Z",
          "iopub.execute_input": "2025-08-24T01:50:15.82052Z",
          "iopub.status.idle": "2025-08-24T01:50:15.866534Z",
          "shell.execute_reply.started": "2025-08-24T01:50:15.820505Z",
          "shell.execute_reply": "2025-08-24T01:50:15.865835Z"
        },
        "id": "MemLThLM8sh2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, classes):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():   # Disable gradient calculation\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)       # Forward pass\n",
        "            _, preds = torch.max(outputs, 1)  # Get class with max probability\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=classes))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:15.86741Z",
          "iopub.execute_input": "2025-08-24T01:50:15.867956Z",
          "iopub.status.idle": "2025-08-24T01:50:15.883037Z",
          "shell.execute_reply.started": "2025-08-24T01:50:15.867935Z",
          "shell.execute_reply": "2025-08-24T01:50:15.882331Z"
        },
        "id": "w2DhBgR48sh2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining ResNet152...\")\n",
        "resnet_trained = train(resnet, trainloader, epochs=5)\n",
        "print(\"\\nEvaluating ResNet152...\")\n",
        "evaluate_model(resnet_trained, testloader, classes)\n",
        "\n",
        "# Train & Evaluate AlexNet\n",
        "print(\"\\nTraining AlexNet...\")\n",
        "alexnet_trained = train(alexnet, trainloader, epochs=5)\n",
        "print(\"\\nEvaluating AlexNet...\")\n",
        "evaluate_model(alexnet_trained, testloader, classes)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T01:50:15.883715Z",
          "iopub.execute_input": "2025-08-24T01:50:15.884078Z",
          "iopub.status.idle": "2025-08-24T02:38:45.985402Z",
          "shell.execute_reply.started": "2025-08-24T01:50:15.884053Z",
          "shell.execute_reply": "2025-08-24T02:38:45.984633Z"
        },
        "id": "lDWnZNp88sh2",
        "outputId": "dda522dc-f583-41b0-91f0-5af3bb8e3162"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nTraining ResNet152...\nEpoch [1/5], Loss: 0.6833\nEpoch [2/5], Loss: 0.5502\nEpoch [3/5], Loss: 0.5326\nEpoch [4/5], Loss: 0.5128\nEpoch [5/5], Loss: 0.5024\n\nEvaluating ResNet152...\nClassification Report:\n              precision    recall  f1-score   support\n\n    airplane       0.82      0.88      0.85      1000\n  automobile       0.89      0.91      0.90      1000\n        bird       0.90      0.73      0.80      1000\n         cat       0.74      0.73      0.74      1000\n        deer       0.74      0.90      0.81      1000\n         dog       0.78      0.85      0.82      1000\n        frog       0.93      0.88      0.91      1000\n       horse       0.91      0.82      0.86      1000\n        ship       0.89      0.89      0.89      1000\n       truck       0.92      0.87      0.90      1000\n\n    accuracy                           0.85     10000\n   macro avg       0.85      0.85      0.85     10000\nweighted avg       0.85      0.85      0.85     10000\n\nConfusion Matrix:\n [[882  13   8  10  15   0   0   8  54  10]\n [ 12 909   1   7   2   3   0   3  16  47]\n [ 44   1 727  52  99  28  26  13   8   2]\n [ 12   7  25 735  57 132  10  11   8   3]\n [  8   0  14  20 897  11  18  27   5   0]\n [  1   3   6  88  27 852   6  14   1   2]\n [  3   3  19  41  33  15 881   3   2   0]\n [ 14   2   6  28  82  41   2 822   3   0]\n [ 70  15   1   9   4   2   1   2 888   8]\n [ 24  65   0   9   4   4   1   4  16 873]]\n\nTraining AlexNet...\nEpoch [1/5], Loss: 1.8748\nEpoch [2/5], Loss: 1.5344\nEpoch [3/5], Loss: 1.3457\nEpoch [4/5], Loss: 1.2492\nEpoch [5/5], Loss: 1.2052\n\nEvaluating AlexNet...\nClassification Report:\n              precision    recall  f1-score   support\n\n    airplane       0.60      0.72      0.65      1000\n  automobile       0.76      0.88      0.81      1000\n        bird       0.61      0.31      0.41      1000\n         cat       0.41      0.41      0.41      1000\n        deer       0.52      0.53      0.53      1000\n         dog       0.54      0.47      0.50      1000\n        frog       0.70      0.68      0.69      1000\n       horse       0.59      0.73      0.65      1000\n        ship       0.74      0.79      0.76      1000\n       truck       0.73      0.72      0.73      1000\n\n    accuracy                           0.62     10000\n   macro avg       0.62      0.62      0.61     10000\nweighted avg       0.62      0.62      0.61     10000\n\nConfusion Matrix:\n [[717  26  18  20  11   5  11  23 137  32]\n [ 25 875   1   2   5   2   5   5  16  64]\n [138   8 308  97 170  89  76  73  26  15]\n [ 53  21  37 411  74 182  78  71  29  44]\n [ 35  10  48  75 532  23  77 175  12  13]\n [ 34   6  40 218  57 472  31  97  22  23]\n [ 13  10  25  98  98  30 680  19  11  16]\n [ 23   6  19  56  59  65   6 728   1  37]\n [ 92  49  12  13   8   5   6   7 785  23]\n [ 66 140   1  12   4   3   6  27  19 722]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}