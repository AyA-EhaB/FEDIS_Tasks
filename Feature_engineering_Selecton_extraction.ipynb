{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzuN9qwSixOmengtm629KQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyA-EhaB/FEDIS_Tasks/blob/main/Feature_engineering_Selecton_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# {1} Feature Extraction\n"
      ],
      "metadata": {
        "id": "ovC5oWTDDLk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target Encoding\n",
        "### Target Encoding: Beyond the Basic Mean (Simplified Explanation)\n",
        "\n",
        "**What is Target Encoding?**\n",
        "\n",
        "* We have categories like “red”, “blue”, “green”.\n",
        "* Instead of using text, we replace each category with a number.\n",
        "* The simplest way: replace it with the **average target value** for that category.\n",
        "\n",
        "**Example (basic mean):**\n",
        "Imagine we have a small online shop dataset where the target is whether the customer bought the product (`1` for yes, `0` for no):\n",
        "\n",
        "```\n",
        "color   bought\n",
        "red     1\n",
        "red     1\n",
        "blue    0\n",
        "green   0\n",
        "red     0\n",
        "blue    1\n",
        "```\n",
        "\n",
        "* \"red\" rows → bought = \\[1, 1, 0] → mean = 0.67\n",
        "* \"blue\" rows → bought = \\[0, 1] → mean = 0.5\n",
        "* \"green\" rows → bought = \\[0] → mean = 0.0\n",
        "\n",
        "We could replace each category with these numbers.\n",
        "\n",
        "**Why the simple way is risky:**\n",
        "\n",
        "* \"green\" appears only once → its mean = 0.0 might be due to chance.\n",
        "* For rare categories, the number is unstable and can mislead the model.\n",
        "\n",
        "**The advanced way — Smoothing (Bayesian blending):**\n",
        "We combine:\n",
        "\n",
        "1. **Category mean** — info from that category.\n",
        "2. **Overall (global) mean** — average target across *all* data points.\n",
        "\n",
        "**What is the global mean here?**\n",
        "In our example:\n",
        "\n",
        "* All bought values: \\[1, 1, 0, 0, 0, 1]\n",
        "* Sum = 3, Count = 6 → global mean = 0.5\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "```\n",
        "encoded = (n_cat * mean_cat + k * mean_global) / (n_cat + k)\n",
        "```\n",
        "\n",
        "Where `k` is how strongly you pull the number towards the global mean.\n",
        "\n",
        "**Real-world analogy:**\n",
        "\n",
        "* Think of movie ratings. If a movie has only 2 reviews with 5 stars each, you wouldn’t trust it as much as a movie with 1,000 reviews averaging 4.8 stars. You’d mix the small-sample score with the average rating of all movies to avoid overreacting.\n",
        "\n",
        "\n",
        "**When to use:**\n",
        "\n",
        "* Many categories (high-cardinality)\n",
        "* Rare categories that could mislead without smoothing\n",
        "* Need to carefully add target info to features\n",
        "\n",
        "**One-line takeaway:**\n",
        "\n",
        "> The overall mean is simply the average target across the entire dataset, used as a safe fallback for categories with few samples.\n",
        "\n"
      ],
      "metadata": {
        "id": "e7HeJjptui_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def target_encode_smooth(train, target, col, k=5):\n",
        "    global_mean = train[target].mean()\n",
        "    agg = train.groupby(col)[target].agg(['mean', 'count'])\n",
        "    smoothing = (agg['count'] * agg['mean'] + k * global_mean) / (agg['count'] + k)\n",
        "    return train[col].map(smoothing)\n",
        "\n",
        "# Example\n",
        "df = pd.DataFrame({\n",
        "    'color': ['red','blue','red','green','green','blue'],\n",
        "    'label': [1,0,1,0,0,1]\n",
        "})\n",
        "df['color_te'] = target_encode_smooth(df, 'label', 'color', k=3)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3_fRa7R8Kea",
        "outputId": "581bdc6f-90e0-4179-9490-6fe41d6b399c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   color  label  color_te\n",
            "0    red      1       0.7\n",
            "1   blue      0       0.5\n",
            "2    red      1       0.7\n",
            "3  green      0       0.3\n",
            "4  green      0       0.3\n",
            "5   blue      1       0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Feature Generation\n",
        "## Method 2 — Polynomial Feature Generation: Capturing Nonlinear Relationships\n",
        "\n",
        "**Definition:**\n",
        "Polynomial feature generation creates new features from existing ones by taking powers and interactions, enabling linear models to approximate nonlinear patterns.\n",
        "\n",
        "**Example:**\n",
        "From features:\n",
        "\n",
        "```\n",
        "X1, X2\n",
        "```\n",
        "\n",
        "We can generate:\n",
        "\n",
        "```\n",
        "X1², X2², X1 × X2\n",
        "```\n",
        "\n",
        "This lets a linear model represent curved trends and feature interactions.\n",
        "\n",
        "**Why not just use nonlinear models?**\n",
        "\n",
        "* **Interpretability:** Linear models with polynomial features remain transparent, unlike many nonlinear methods.\n",
        "* **Small datasets:** Nonlinear models often require more data to perform well.\n",
        "* **Efficiency:** Polynomial features keep training fast and memory use low.\n",
        "* **Compliance:** In some industries (finance, healthcare), interpretable models are legally preferred.\n",
        "\n",
        "**Real-world applications:**\n",
        "\n",
        "* **House prices:** Price might peak at an optimal number of rooms; adding `rooms²` allows a linear model to capture this “sweet spot” curve.\n",
        "* **Physics:** Distance = speed × time can be created as a polynomial interaction feature.\n",
        "* **Marketing:** Sales rise with ad spend but level off (diminishing returns). Adding `ad_spend²` allows a linear model to fit a curve that rises quickly, then flattens.\n",
        "\n",
        "**Risks:**\n",
        "\n",
        "* High-degree polynomials can explode feature count, causing overfitting (“curse of dimensionality”).\n",
        "\n",
        "**Advanced use:**\n",
        "\n",
        "* Create only plausible features based on domain expertise.\n",
        "* Use feature selection methods such as Recursive Feature Elimination (RFE) or LASSO regression to keep only the most useful polynomial terms.\n",
        "\n",
        "**When to use:**\n",
        "\n",
        "* You suspect nonlinear relationships.\n",
        "* You need interpretable models.\n",
        "* You have limited data but want richer features.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "> Polynomial features give linear models the flexibility to model curves and interactions without abandoning simplicity.\n"
      ],
      "metadata": {
        "id": "ghmqd9N0El3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Sample data\n",
        "df = pd.DataFrame({\n",
        "    'X1': [1, 2, 3],\n",
        "    'X2': [4, 5, 6]\n",
        "})\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
        "poly_features = poly.fit_transform(df)\n",
        "feature_names = poly.get_feature_names_out(['X1', 'X2'])\n",
        "\n",
        "poly_df = pd.DataFrame(poly_features, columns=feature_names)\n",
        "print(poly_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLHqtl808ZDp",
        "outputId": "55333c75-5b77-4881-9bc2-c3816ad07628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    X1   X2  X1^2  X1 X2  X2^2\n",
            "0  1.0  4.0   1.0    4.0  16.0\n",
            "1  2.0  5.0   4.0   10.0  25.0\n",
            "2  3.0  6.0   9.0   18.0  36.0\n"
          ]
        }
      ]
    }
  ]
}